{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_DMN7Nl4fqNL"
      },
      "source": [
        "## **CREATION OF LONDON GRAPH USING OPENSTREETMAP AND PRINT USEFUL INFORMATION** ##"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 562
        },
        "id": "VSAAyVT_rf6X",
        "outputId": "e254ef77-2d77-431b-bd9a-21a5ab8e7649"
      },
      "outputs": [],
      "source": [
        "import osmnx as ox\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# We set the location as London and download the graph\n",
        "location = \"London, United Kingdom\"\n",
        "\n",
        "graph = ox.graph_from_place(location, network_type=\"drive\")\n",
        "\n",
        "ox.io.save_graphml(graph, filepath=\"london_graph.graphml\")\n",
        "\n",
        "# We obtain the information about the graph\n",
        "num_nodes = nx.number_of_nodes(graph)\n",
        "num_edges = nx.number_of_edges(graph)\n",
        "average_degree = sum(dict(graph.degree()).values()) / num_nodes\n",
        "\n",
        "# Prints the values found, plots the graph, and saves the graph\n",
        "print(\"# of node:\", num_nodes)\n",
        "print(\"# of edge :\", num_edges)\n",
        "print(\"average degree:\", average_degree)\n",
        "ox.plot_graph(ox.project_graph(graph), show=False)\n",
        "plt.savefig(\"london_graph.png\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IFibYhcAxJIh"
      },
      "source": [
        "## **GRAPH FEATURES**\n",
        "Let's plot the features related to the nodes and edges of the London graph. Additionally, we will compute the degree centrality for each node and add it as a new feature for the nodes.  The degree centrality is defined as:\n",
        "$$\n",
        "\n",
        "DegreeCentrality(v) = \\frac{d_v}{|N|-1}\n",
        "$$\n",
        "\n",
        "where $ d_v$ is the degree of the node $v$ that is the number of edges that converge on it and $ N $ is the number of nodes of the London graph.\n",
        "In this context, the degree centrality measures the importance of a cross in terms of how many streets converge on it, and it is computed for the entire graph. Therefore, a node with a high degree of centrality is not just important locally, but is considered significant in the broader context of the entire street network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vyUYUcaGB372",
        "outputId": "8a6eb6b9-8d37-4dbd-bb3c-dd4e343fed76"
      },
      "outputs": [],
      "source": [
        "# We calculate the centrality for each node and add it as a node attribute\n",
        "degree_centrality = nx.degree_centrality(graph)\n",
        "nx.set_node_attributes(graph, degree_centrality, \"degree_centrality\")\n",
        "\n",
        "# We obtain and display the features\n",
        "gdf_nodes, gdf_edges = ox.graph_to_gdfs(graph)\n",
        "print(\"Node Features:\")\n",
        "print(gdf_nodes.head())\n",
        "print(\"\\nEdge Features:\")\n",
        "print(gdf_edges.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qDNIn_SvxNn_"
      },
      "source": [
        "## **LOAD THE DATASET UK_ACCIDENT + FEATURES**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dzYEAZmks-h7",
        "outputId": "84339395-e13f-474e-c45c-141eb25c0afd"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the UK-Accidents dataset from the csv file\n",
        "uk_accident_data = pd.read_csv('UK_Accident.csv')\n",
        "\n",
        "# Print the features of the dataset\n",
        "print(\"Features of the UK-Accidents dataset:\")\n",
        "print(uk_accident_data.columns)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **PREPROCESSING OF THE DATASET UK_ACCIDENT**\n",
        "We are only interested in ensuring that the features useful to create the new dataset do not contain any NaN values. Therefore, we will remove all the rows that have NaN values. We can see that the dataset only had NaN values for the feature 'Longitude' and that 101 rows have been removed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# We count the NaN values present in the interested features before the filtering\n",
        "nan_count1 = uk_accident_data['Longitude'].isnull().sum()\n",
        "nan_count2 = uk_accident_data['Latitude'].isnull().sum()\n",
        "nan_count3 = uk_accident_data['Number_of_Casualties'].isnull().sum()\n",
        "nan_count4 = uk_accident_data['Accident_Severity'].isnull().sum()\n",
        "nan_count5 = uk_accident_data['Speed_limit'].isnull().sum()\n",
        "\n",
        "# We print the counts before the filtering\n",
        "print(\"Number of NaN values BEFORE the filtering\")\n",
        "print(f\"The Nan values in the 'Longitude' column are: {nan_count1}\")\n",
        "print(f\"The Nan values in the 'Latitude' column are: {nan_count2}\")\n",
        "\n",
        "print(f\"The Nan values in the 'Number_of_Casualties' column are: {nan_count3}\")\n",
        "print(f\"The Nan values in the column 'Accident_Severity' are: {nan_count4}\")\n",
        "print(f\"The Nan values in the 'Speed_limit' column are: {nan_count5}\")\n",
        "print(\"\\n\")\n",
        "\n",
        "# We remove rows with missing values in the columns\n",
        "uk_accident_filtered_data = uk_accident_data.dropna(subset=['Longitude', 'Latitude','Number_of_Casualties', 'Accident_Severity', 'Speed_limit'])\n",
        "\n",
        "# We count the NaN values present in the interested features after the filtering\n",
        "nan_count1 = uk_accident_filtered_data['Longitude'].isnull().sum()\n",
        "nan_count2 = uk_accident_filtered_data['Latitude'].isnull().sum()\n",
        "nan_count3 = uk_accident_data['Number_of_Casualties'].isnull().sum()\n",
        "nan_count4 = uk_accident_data['Accident_Severity'].isnull().sum()\n",
        "nan_count5 = uk_accident_data['Speed_limit'].isnull().sum()\n",
        "\n",
        "# We print the counts before the filtering\n",
        "print(\"Number of NaN values AFTER the filtering\")\n",
        "print(f\"The Nan values in the 'Longitude' column are: {nan_count1}\")\n",
        "print(f\"The Nan values in the 'Latitude' column are: {nan_count2}\")\n",
        "\n",
        "print(f\"The Nan values in the 'Number_of_Casualties' column are: {nan_count3}\")\n",
        "print(f\"The Nan values in the column 'Accident_Severity' are: {nan_count4}\")\n",
        "print(f\"The Nan values in the 'Speed_limit' column are: {nan_count5}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6DfPhdBHxgkO"
      },
      "source": [
        "## **CONVERSION OF GRAPH DATA TO A CSV FILE**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FHpO_2OlxaUX"
      },
      "outputs": [],
      "source": [
        "# We convert the OSM data to a Pandas dataframe regarding nodes and save it to a new cvs file\n",
        "# This conversion allows us to manage better the data of the graph\n",
        "nodes = ox.graph_to_gdfs(graph, edges=False)\n",
        "nodes.to_csv(\"london_osm_nodes.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jntu9STGxnZG"
      },
      "source": [
        "## **CREATION OF THE NEW DATASET USED FOR THE AVERAGE SEVERITY CLASSIFICATION TASK**\n",
        "\n",
        "The goal was to create a new dataset combining the features of the nodes of the London graph with some features of the UK accident dataset. The accident features that we found useful for our classification task were: longitude, latitude, number of casualties, speed limit, and the severity of the accident. To accomplish the creation of the new dataset, we needed to find the nearest cross for each accident which required a lot of computation. To perform this operation efficiently, we used the Ball Tree data structure, which is suitable for nearest-neighbor searches. The decision to use the Ball Tree came after several attempts in which we used the haversine distance and the shortest path to find the nearest accidents for each node.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 418
        },
        "id": "uDqYMdtshoKe",
        "outputId": "271492e2-a625-476e-a140-ec2237045fcd"
      },
      "outputs": [],
      "source": [
        "import geopandas as gpd\n",
        "from shapely.geometry import Point\n",
        "from sklearn.neighbors import BallTree\n",
        "\n",
        "# We load the data from the London csv file\n",
        "osm_data = pd.read_csv('london_osm_nodes.csv')\n",
        "\n",
        "# We extract x and y columns from osm_data and creating Point geometries, this operation allows us to convert\n",
        "# raw coordinate data into a spatial format that is suitable to perform spatial operations.\n",
        "osm_data['geometry'] = osm_data.apply(lambda row: Point(row['x'], row['y']), axis=1)\n",
        "\n",
        "# We create a GeoDataFrame from osm_data with 'geometry' column as the geometry attribute. This data structure is specifically designed\n",
        "# for handling geospatial data and allows you to perform spatial operations and queries.\n",
        "osm_gdf = gpd.GeoDataFrame(osm_data, geometry='geometry')\n",
        "\n",
        "# Create the Ball Tree (spatial) search tree using the 'x' and 'y' coordinates of osm_gdf\n",
        "tree = BallTree(osm_gdf[['x', 'y']].values, leaf_size=10)\n",
        "\n",
        "# Query the Ball Tree to find the nearest node (cross) for each accident based on 'Longitude' and 'Latitude'\n",
        "nearest_accidents = tree.query(uk_accident_filtered_data[['Longitude', 'Latitude']].values, k=1, return_distance=False)\n",
        "\n",
        "# Add a new column 'Nearest_Node' to uk_accident_filtered_data, storing the index of the nearest node for each accident\n",
        "uk_accident_filtered_data.loc[:, 'Nearest_Node'] = nearest_accidents.flatten()\n",
        "\n",
        "# Calculate the average number of casualties for each nearest node\n",
        "node_avg_casualties = uk_accident_filtered_data.groupby('Nearest_Node')['Number_of_Casualties'].mean()\n",
        "\n",
        "# Calculate the average accident severity for each nearest node\n",
        "node_avg_severities = uk_accident_filtered_data.groupby('Nearest_Node')['Accident_Severity'].mean()\n",
        "\n",
        "# Calculate the average speed limit for each nearest node\n",
        "node_avg_speeds = uk_accident_filtered_data.groupby('Nearest_Node')['Speed_limit'].mean()\n",
        "\n",
        "# Create a new dataset using selected columns from osm_gdf\n",
        "new_dataset = osm_gdf[['x', 'y', 'street_count', 'degree_centrality']].copy()\n",
        "\n",
        "# Rename columns for better clarity and consistency\n",
        "new_dataset.rename(columns={'x': 'longitude', 'y': 'latitude'}, inplace=True)\n",
        "\n",
        "# Calculate the number of nearest accidents for each node in the new dataset\n",
        "new_dataset['number_of_nearest_accidents'] = osm_gdf.index.map(lambda node_id: len(uk_accident_filtered_data[uk_accident_filtered_data['Nearest_Node'] == node_id]))\n",
        "\n",
        "# Map the average number of casualties to each node in the new dataset\n",
        "new_dataset['average_number_of_casualties'] = osm_gdf.index.map(lambda node_id: node_avg_casualties.get(node_id, 0))\n",
        "\n",
        "# Map the average speed limits to each node in the new dataset\n",
        "new_dataset['average_speed_limits'] = osm_gdf.index.map(lambda node_id: node_avg_speeds.get(node_id, 0))\n",
        "\n",
        "# Map the average accident severity to each node in the new dataset\n",
        "new_dataset['average_accident_severity'] = osm_gdf.index.map(lambda node_id: node_avg_severities.get(node_id, 0))\n",
        "\n",
        "# We save and load the new dataset\n",
        "new_dataset.to_csv('new_dataset.csv', index=False)\n",
        "\n",
        "#we check if the new dataset contains missing values\n",
        "nan_count1 = new_dataset.isnull().any(axis=1).sum()\n",
        "\n",
        "if nan_count1 != 0:\n",
        "    # Delete rows that contain at least a Nan value in some feature\n",
        "    new_dataset = new_dataset.dropna()\n",
        "else:\n",
        "    print(f\"The new dataset does not contain missing values: {nan_count1}\")\n",
        "\n",
        "print(\"\\n\")\n",
        "\n",
        "# We print the number of records of the new dataset\n",
        "num_records = len(new_dataset)\n",
        "print(\"The new dataset contains\", num_records, \"record.\")\n",
        "\n",
        "# We print the features of the rows present in the dataset\n",
        "print(\"Features of the new dataset:\")\n",
        "print(new_dataset.columns)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **SPLIT THE NEW DATASET IN TRAINING, VALIDATION, TEST SET** "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Load the dataset and convert the regression problem into a classification problem\n",
        "label_encoder = LabelEncoder()\n",
        "new_dataset['severity_category'] = label_encoder.fit_transform(pd.cut(new_dataset['average_accident_severity'], bins=[-float('inf'), 1, 2, float('inf')], labels=['low', 'medium', 'high']))\n",
        "\n",
        "# We define features X and target y\n",
        "X = new_dataset[['longitude', 'latitude', 'street_count', 'degree_centrality','number_of_nearest_accidents', 'average_number_of_casualties', 'average_speed_limits']]\n",
        "y = new_dataset['severity_category']\n",
        "\n",
        "# We split the data into training, validation and testing sets\n",
        "X_train_pre, X_temp_pre, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "X_val_pre, X_test_pre, y_val, y_test = train_test_split(X_temp_pre, y_temp, test_size=0.33, random_state=42)\n",
        "\n",
        "print(f\"Number of samples in the training set: {len(X_train_pre)}\")\n",
        "print(f\"Number of samples in the validation set: {len(X_val_pre)}\")\n",
        "print(f\"Number of samples in the test set: {len(X_test_pre)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Standardize the data**\n",
        "Before training the models, the data in the datasets are normalized\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn import preprocessing\n",
        "\n",
        "scaler = preprocessing.StandardScaler()\n",
        "scaler.fit(X_train_pre)\n",
        "\n",
        "X_train = scaler.transform(X_train_pre)\n",
        "X_val = scaler.transform(X_val_pre)\n",
        "X_temp = scaler.transform(X_test_pre)\n",
        "X_test = scaler.transform(X_test_pre)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5iem-5ogx2pW"
      },
      "source": [
        "## **SVM**\n",
        "We decided to use the SVM classifier with a Radial Basis Function (RBF) kernel and we performed a hyperparameter tuning using Grid Search. The hyperparameter grid included variations of the regularization parameter C and the gamma parameter for the RBF kernel. The SVM model was trained on the training data with different hyperparameter combinations, and the best-performing model was selected based on cross-validated performance. In the end, we evaluated the model with the best hyperparameters on the validation and test set.\n",
        "Tests were also performed with different types of kernels (polynomial and sigmoid), but RBF is the one that obtained the best results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8XBkx_AvmHNN",
        "outputId": "4b3c7678-24fc-49e5-a06b-a6c5264e43ed"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Define the parameter grid for hyperparameter tuning\n",
        "param_grid = {'C': [0.1, 1, 10],\n",
        "              'gamma': [0.01, 0.1, 1],\n",
        "              'kernel': ['rbf']}\n",
        "\n",
        "# Create the SVM model\n",
        "svm_model = SVC(random_state=42)\n",
        "\n",
        "# Set up GridSearchCV\n",
        "grid_search = GridSearchCV(svm_model, param_grid, cv=5, n_jobs=7)\n",
        "\n",
        "# Fit the model to the training data and find the best hyperparameters\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best hyperparameters\n",
        "best_params = grid_search.best_params_\n",
        "\n",
        "# Use the best model for predictions on the validation set\n",
        "best_svm_model = grid_search.best_estimator_\n",
        "y_val_pred = best_svm_model.predict(X_val)\n",
        "\n",
        "# Evaluation on the validation set\n",
        "accuracy = accuracy_score(y_val, y_val_pred)\n",
        "precision = precision_score(y_val, y_val_pred, average='weighted')\n",
        "recall = recall_score(y_val, y_val_pred, average='weighted')\n",
        "f1 = f1_score(y_val, y_val_pred, average='weighted')\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"Best Hyperparameters:\", best_params)\n",
        "print(f\"\\nEvaluation on the Validation Set:\")\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")\n",
        "\n",
        "# Make predictions on the test set with the best model\n",
        "y_test_pred = best_svm_model.predict(X_test)\n",
        "\n",
        "# Evaluation on the test set\n",
        "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
        "test_precision = precision_score(y_test, y_test_pred, average='weighted')\n",
        "test_recall = recall_score(y_test, y_test_pred, average='weighted')\n",
        "test_f1 = f1_score(y_test, y_test_pred, average='weighted')\n",
        "\n",
        "# Print the evaluation metrics on the test set\n",
        "print(\"\\nEvaluation on the Test Set:\")\n",
        "print(f\"Accuracy: {test_accuracy:.4f}\")\n",
        "print(f\"Precision: {test_precision:.4f}\")\n",
        "print(f\"Recall: {test_recall:.4f}\")\n",
        "print(f\"F1 Score: {test_f1:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Tests with other kernels: polynomial and sigmoid. \n",
        "They are commented because they take much longer to execute while also obtaining worse results than the RBF kernel.\n",
        "The code is working and usable when removed from the commented block"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\" \n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Test with polynomial kernel\n",
        "\n",
        "# Define the parameter grid for hyperparameter tuning\n",
        "param_grid = {'C': [0.1, 1, 10],\n",
        "              'degree': [2,3],\n",
        "              'kernel': ['poly']}\n",
        "\n",
        "# Create the SVM model\n",
        "svm_model = SVC(random_state=42)\n",
        "\n",
        "# Set up GridSearchCV\n",
        "grid_search = GridSearchCV(svm_model, param_grid, cv=5, n_jobs=7)\n",
        "\n",
        "# Fit the model to the training data and find the best hyperparameters\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best hyperparameters\n",
        "best_params = grid_search.best_params_\n",
        "\n",
        "# Use the best model for predictions on the validation set\n",
        "best_svm_model = grid_search.best_estimator_\n",
        "y_val_pred = best_svm_model.predict(X_val)\n",
        "\n",
        "# Evaluation on the validation set\n",
        "accuracy = accuracy_score(y_val, y_val_pred)\n",
        "precision = precision_score(y_val, y_val_pred, average='weighted')\n",
        "recall = recall_score(y_val, y_val_pred, average='weighted')\n",
        "f1 = f1_score(y_val, y_val_pred, average='weighted')\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"Best Hyperparameters:\", best_params)\n",
        "print(f\"\\nEvaluation on the Validation Set:\")\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")\n",
        "\n",
        "# Make predictions on the test set with the best model\n",
        "y_test_pred = best_svm_model.predict(X_test)\n",
        "\n",
        "# Evaluation on the test set\n",
        "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
        "test_precision = precision_score(y_test, y_test_pred, average='weighted')\n",
        "test_recall = recall_score(y_test, y_test_pred, average='weighted')\n",
        "test_f1 = f1_score(y_test, y_test_pred, average='weighted')\n",
        "\n",
        "# Print the evaluation metrics on the test set\n",
        "print(\"\\nEvaluation on the Test Set:\")\n",
        "print(f\"Accuracy: {test_accuracy:.4f}\")\n",
        "print(f\"Precision: {test_precision:.4f}\")\n",
        "print(f\"Recall: {test_recall:.4f}\")\n",
        "print(f\"F1 Score: {test_f1:.4f}\")\n",
        " \"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\" from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Test with the sigmoid kernel\n",
        "\n",
        "# Define the parameter grid for hyperparameter tuning\n",
        "param_grid = {'C': [0.1, 1, 10],\n",
        "              'gamma': [0.1],\n",
        "              'coef0':[0,1],\n",
        "              'kernel': ['sigmoid']}\n",
        "\n",
        "# Create the SVM model\n",
        "svm_model = SVC(random_state=42)\n",
        "\n",
        "# Set up GridSearchCV\n",
        "grid_search = GridSearchCV(svm_model, param_grid, cv=5, n_jobs=7)\n",
        "\n",
        "# Fit the model to the training data and find the best hyperparameters\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best hyperparameters\n",
        "best_params = grid_search.best_params_\n",
        "\n",
        "# Use the best model for predictions on the validation set\n",
        "best_svm_model = grid_search.best_estimator_\n",
        "y_val_pred = best_svm_model.predict(X_val)\n",
        "\n",
        "# Evaluation on the validation set\n",
        "accuracy = accuracy_score(y_val, y_val_pred)\n",
        "precision = precision_score(y_val, y_val_pred, average='weighted')\n",
        "recall = recall_score(y_val, y_val_pred, average='weighted')\n",
        "f1 = f1_score(y_val, y_val_pred, average='weighted')\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"Best Hyperparameters:\", best_params)\n",
        "print(f\"\\nEvaluation on the Validation Set:\")\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")\n",
        "\n",
        "# Make predictions on the test set with the best model\n",
        "y_test_pred = best_svm_model.predict(X_test)\n",
        "\n",
        "# Evaluation on the test set\n",
        "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
        "test_precision = precision_score(y_test, y_test_pred, average='weighted')\n",
        "test_recall = recall_score(y_test, y_test_pred, average='weighted')\n",
        "test_f1 = f1_score(y_test, y_test_pred, average='weighted')\n",
        "\n",
        "# Print the evaluation metrics on the test set\n",
        "print(\"\\nEvaluation on the Test Set:\")\n",
        "print(f\"Accuracy: {test_accuracy:.4f}\")\n",
        "print(f\"Precision: {test_precision:.4f}\")\n",
        "print(f\"Recall: {test_recall:.4f}\")\n",
        "print(f\"F1 Score: {test_f1:.4f}\")\n",
        " \"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZmhZsRSx6nQ"
      },
      "source": [
        "## **RANDOM FOREST**\n",
        "Let's train and evaluate the Random Forest classifier. Additionally, in this case, we performed a hyperparameter tuning\n",
        "using Grid Search. The hyperparameters to be tuned are:\n",
        "1. **Number of estimators**: number of trees in the forest.\n",
        "2. **max_depth**: maximum depth of the individual decision trees.\n",
        "3. **min samples split**: minimum number of samples required to split an internal node.\n",
        "4. **min_samples_leaf**: minimum number of samples required to be a leaf node.\n",
        "5. **max_features**: the number of features that are taken into account for each split while building an individual decision tree. In our case, we considered two options: the *sqrt* or the *log2* over the total number of features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7XJVV2lvoi5l",
        "outputId": "f98ccbe8-c0ad-4a6c-cefa-28e4ee0e0004"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Define the parameter grid for hyperparameter tuning\n",
        "param_grid = {'n_estimators': [100, 200],\n",
        "              'max_depth': [None, 10],\n",
        "              'min_samples_split': [2, 5],\n",
        "              'min_samples_leaf': [1, 2],\n",
        "              'max_features': ['sqrt', 'log2']}\n",
        "\n",
        "# Create the RandomForest model\n",
        "rf_model = RandomForestClassifier(random_state=42, n_jobs=6)\n",
        "\n",
        "# Set up GridSearchCV\n",
        "grid_search = GridSearchCV(rf_model, param_grid, cv=5, n_jobs=6)\n",
        "\n",
        "# Fit the model to the training data and find the best hyperparameters\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best hyperparameters\n",
        "best_params = grid_search.best_params_\n",
        "\n",
        "# Use the best model for predictions on the validation set\n",
        "best_rf_model = grid_search.best_estimator_\n",
        "y_val_pred = best_rf_model.predict(X_val)\n",
        "\n",
        "# Evaluation on the validation set\n",
        "accuracy = accuracy_score(y_val, y_val_pred)\n",
        "precision = precision_score(y_val, y_val_pred, average='weighted')\n",
        "recall = recall_score(y_val, y_val_pred, average='weighted')\n",
        "f1 = f1_score(y_val, y_val_pred, average='weighted')\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"Best Hyperparameters:\", best_params)\n",
        "print(f\"\\nEvaluation on the Validation Set:\")\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")\n",
        "\n",
        "# Make predictions on the test set with the best model\n",
        "y_test_pred = best_rf_model.predict(X_test)\n",
        "\n",
        "# Evaluation on the test set\n",
        "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
        "test_precision = precision_score(y_test, y_test_pred, average='weighted')\n",
        "test_recall = recall_score(y_test, y_test_pred, average='weighted')\n",
        "test_f1 = f1_score(y_test, y_test_pred, average='weighted')\n",
        "\n",
        "# Print the evaluation metrics on the test set\n",
        "print(\"\\nEvaluation on the Test Set:\")\n",
        "print(f\"Accuracy: {test_accuracy:.4f}\")\n",
        "print(f\"Precision: {test_precision:.4f}\")\n",
        "print(f\"Recall: {test_recall:.4f}\")\n",
        "print(f\"F1 Score: {test_f1:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7za_RbgYx-Ti"
      },
      "source": [
        "## **KNN**\n",
        "In our case, we considered three k-nearest neighbors classifiers with 3 different values of k. In the end, we chose the model that obtained the best accuracy value on the validation set.\n",
        "We did not use hyperparameter tuning using Grid Search because we found that the final results were slightly worse compared to the procedure that we used."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yy3J9Y2t1Qs1",
        "outputId": "3cb8f9dd-fa6c-4ab9-ad07-b4f11741817e"
      },
      "outputs": [],
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "# List of k values to test\n",
        "k_values = [3, 5, 7]\n",
        "\n",
        "# Dictionary to store models and their performances\n",
        "models_performance = {}\n",
        "\n",
        "# Now, we create create, train and evaluate models for each k value\n",
        "for k in k_values:\n",
        "\n",
        "    # We create the model using a specific value of k\n",
        "    knn_model = KNeighborsClassifier(n_neighbors=k)\n",
        "    \n",
        "    knn_model.fit(X_train, y_train)\n",
        "\n",
        "    # We make predictions on the validation set\n",
        "    y_val_pred = knn_model.predict(X_val)\n",
        "\n",
        "    # Evaluation\n",
        "    accuracy = accuracy_score(y_val, y_val_pred)\n",
        "\n",
        "    # Store the model and its accuracy measured on validation set\n",
        "    models_performance[k] = {'model': knn_model, 'accuracy': accuracy}\n",
        "\n",
        "\n",
        "\n",
        "for k in k_values:\n",
        "    knn_model = models_performance[k]['model']\n",
        "    knn_accuracy = models_performance[k]['accuracy']\n",
        "\n",
        "    # Print the accuracy of the best model on the validation set\n",
        "    print(f\"\\nKNN model with (k={k}):\")\n",
        "    print(f\"Accuracy on Validation Set: {knn_accuracy:.4f}\")\n",
        "\n",
        "    # Make predictions on the test set with the best model\n",
        "    y_test_pred = knn_model.predict(X_test)\n",
        "\n",
        "    # Evaluation of the best model\n",
        "    test_accuracy = accuracy_score(y_test, y_test_pred)\n",
        "    test_precision = precision_score(y_test, y_test_pred, average='weighted')\n",
        "    test_recall = recall_score(y_test, y_test_pred, average='weighted')\n",
        "    test_f1 = f1_score(y_test, y_test_pred, average='weighted')\n",
        "\n",
        "    # Print performance on test set\n",
        "    print(f\"\\nEvaluation on the Test Set using KNN model with (k={k})\")\n",
        "    print(f\"Accuracy: {test_accuracy:.4f}\")\n",
        "    print(f\"Precision: {test_precision:.4f}\")\n",
        "    print(f\"Recall: {test_recall:.4f}\")\n",
        "    print(f\"F1 Score: {test_f1:.4f}\")\n",
        "\n",
        "\n",
        "\n",
        "# Find the model with the highest accuracy\n",
        "best_k = max(models_performance, key=lambda k: models_performance[k]['accuracy'])\n",
        "\n",
        "best_accuracy = models_performance[best_k]['accuracy']\n",
        "\n",
        "print(f\"\\nBased on the validation accuracy the best model is the KNN with (k={best_k})\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RGnZM-2ZL5ip"
      },
      "source": [
        "## **CREATION OF THE GRAPH FOR THE NEW DATASET**\n",
        "This step is necessary because once we have the graph corresponding to the new dataset, we can transform the graph into a PyTorch Geometric object that is in the right format for the graph neural network. As you can see, the information about the new graph is the same as that on the London graph. The only difference is that the nodes contain the features that we added. As you can see, the information about the new graph is the same as that on the London graph. The only difference is that the nodes contain the features that we added."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5abSumc4MOmt",
        "outputId": "566c3a1a-ae62-4203-d9b3-752600cb10de"
      },
      "outputs": [],
      "source": [
        "import networkx as nx\n",
        "\n",
        "# We create the Dataset dictionary that creates pairs (count, attributes) for the new dataset\n",
        "count_to_coordinates = {}\n",
        "count = 0\n",
        "\n",
        "for idx, row in new_dataset.iterrows():\n",
        "    attributes = (row['number_of_nearest_accidents'], row['average_number_of_casualties'],row['average_speed_limits'], row['severity_category'])\n",
        "    count_to_coordinates[count] = attributes\n",
        "    count += 1\n",
        "\n",
        "# We create the Graph dictionary that create pairs (node_id, count)\n",
        "node_id_to_index = {}\n",
        "index_counter = 0\n",
        "\n",
        "for node_id, node_data in graph.nodes(data=True):\n",
        "    node_id_to_index[node_id] = index_counter\n",
        "    index_counter += 1\n",
        "\n",
        "# for each node_id we get a count from the Graph dictionary\n",
        "# We use the count to get attributes of the node with the specific node_id ( using the Dataset dictionary)\n",
        "for node_id, node_data in graph.nodes(data=True):\n",
        "    \n",
        "    dataset_index = node_id_to_index[node_id]\n",
        "    attributes = count_to_coordinates[dataset_index]\n",
        "\n",
        "    graph.nodes[node_id]['number_of_nearest_accidents'] = attributes[0]\n",
        "    graph.nodes[node_id]['average_number_of_casualties'] = attributes[1]\n",
        "    graph.nodes[node_id]['average_speed_limits'] = attributes[2]\n",
        "    graph.nodes[node_id]['severity_category'] = attributes[3]\n",
        "\n",
        "\n",
        "num_nodes = nx.number_of_nodes(graph)\n",
        "num_edges = nx.number_of_edges(graph)\n",
        "\n",
        "average_degree = sum(dict(graph.degree()).values()) / num_nodes\n",
        "\n",
        "# Print some informations\n",
        "print(\"# of node:\", num_nodes)\n",
        "print(\"# of edge :\", num_edges)\n",
        "print(\"average degree:\", average_degree)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "--E6oeupef-4"
      },
      "source": [
        "## **PRINT FEATURES ABOUT THE NEW GRAPH**\n",
        "\n",
        "The first three features of the nodes are the same of the ones of London graph and the others are the extra features computed using UK accident dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9oN5dQjce3a3",
        "outputId": "6f3e711d-51d3-4ed8-8270-cf2d25eb8e27"
      },
      "outputs": [],
      "source": [
        "# We remove the 'highway' attribute\n",
        "for node_id in graph.nodes():\n",
        "    if 'highway' in graph.nodes[node_id]:\n",
        "        del graph.nodes[node_id]['highway']\n",
        "\n",
        "for node_id in graph.nodes():\n",
        "    if 'ref' in graph.nodes[node_id]:\n",
        "        del graph.nodes[node_id]['ref']\n",
        "\n",
        "# We create a dataframe with all the features of the nodes\n",
        "node_df = pd.DataFrame.from_dict(dict(graph.nodes(data=True)), orient='index')\n",
        "\n",
        "print(node_df.head())\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **DEFINITION OF THE GRAPH NEURAL NETWORK MODEL**\n",
        "For our classification task we have considered a GCN model that consists of two graph convolutional layers. The first layer takes an input feature size of input_size and transforms it to a hidden representation of size hidden_size. The second layer further transforms the hidden representation into an output of size num_classes, which corresponds to the number of unique classes that in our case is 3 (0,1,2).\n",
        "This model introduces non-linearity by using the rectified linear unit (ReLU) activation function and uses the regularization technique dropout to prevent overfitting  by randomly \"dropping out\" a fraction of the neurons during the training phase. The optimization is performed using the Adam optimizer with a learning rate of 0.01 and weight decay of 5e-4. The loss function employed is the cross-entropy loss suitable for multiclass problems."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch_geometric.nn import GCNConv\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import TensorDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# We define the our GNN model\n",
        "class GCN(torch.nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_classes):\n",
        "        super().__init__()\n",
        "        torch.manual_seed(1234567)\n",
        "        self.conv1 = GCNConv(input_size, hidden_size)\n",
        "        self.conv2 = GCNConv(hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = x.relu()\n",
        "        x = F.dropout(x, p=0.5, training=self.training)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return x\n",
        "\n",
        "# Model initialization + loss function + optimizer\n",
        "input_size = 7 \n",
        "hidden_size = 133\n",
        "num_classes = len(torch.unique(torch.tensor(new_dataset['severity_category'])))\n",
        "model = GCN(input_size, hidden_size, num_classes)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
        "#optimizer = torch.optim.RMSprop(model.parameters(), lr=0.01, alpha=0.9, weight_decay=5e-4)\n",
        "#optimizer = torch.optim.Adagrad(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
        "\n",
        "\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "print(model)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **GNN LEARNING APPROACH**\n",
        "To learn and evaluate the graph neural network, we split the new graph into three subgraphs: training, validation, and test. Then transform them into three different PyTorch Geometric objects ready to be used as input for the GNN. Moreover, we considered two versions of the code:\n",
        "1. We did not divide the training nodes into batches.\n",
        "2. We divided the nodes of the training subgraph into batches of 133 samples.\n",
        "\n",
        "In both cases, we used 200 epochs for the training phases."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **SPLIT THE NEW GRAPH IN THREE SUBGRAPHS**\n",
        "\n",
        "Now, we randomly split the nodes of the new graph and define 3 different subgraphs using these nodes.\n",
        "For further clarification, we visualize the corresponding subgraphs in an image.\n",
        "The green nodes represent the training subgraph, the blue nodes are for the validation subgraph, and the orange nodes are for the test subgraph."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.image as mpimg\n",
        "\n",
        "# Get the list of graph nodes\n",
        "nodes_list = list(graph.nodes)\n",
        "\n",
        "# Division into training, validation and test sets\n",
        "train_nodes, temp_nodes = train_test_split(nodes_list, test_size=0.3, random_state=42)\n",
        "val_nodes, test_nodes = train_test_split(temp_nodes, test_size=0.33, random_state=42)\n",
        "\n",
        "# Creation of sub-graphs\n",
        "train_graph = graph.subgraph(train_nodes)\n",
        "val_graph = graph.subgraph(val_nodes)\n",
        "test_graph = graph.subgraph(test_nodes)\n",
        "\n",
        "# Plot the entire graph with node colors based on subgraph membership\n",
        "fig, ax = ox.plot_graph(ox.project_graph(graph), show=False, node_color='gray', node_size=1)\n",
        "\n",
        "# Update node colors for training, validation, and test subgraphs\n",
        "node_colors = ['green' if node in train_nodes else 'blue' if node in val_nodes else 'orange' for node in graph.nodes]\n",
        "\n",
        "# Plot the subgraphs with updated node colors\n",
        "# Training subgraph: green nodes\n",
        "# Validation subgraph: blue nodes\n",
        "# Test subgraph: orange nodes\n",
        "fig, ax = ox.plot_graph(ox.project_graph(train_graph), ax=ax, show=False, node_color='green', node_size=2)\n",
        "fig, ax = ox.plot_graph(ox.project_graph(val_graph), ax=ax, show=False, node_color='blue', node_size=2)\n",
        "fig, ax = ox.plot_graph(ox.project_graph(test_graph), ax=ax, show=False, node_color='orange', node_size=2)\n",
        "\n",
        "# Save the figure\n",
        "plt.savefig(\"london_graph_colored.png\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WwXJQMqZhRiQ"
      },
      "source": [
        "## **CONVERT THE GRAPH INTO A PYTORCH GEOMETRIC DATA OBJECT**\n",
        "In this part, we defined the function to transform the subgraphs and the entire graph into a PyTorch Geometric object that is the suitable format for the GNN."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PnZmkd1Khby4",
        "outputId": "fd7a80eb-4246-45e8-e05d-8e2609144e41"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch_geometric.data import Data\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Function to convert a subgraph to a DataFrame\n",
        "def graph_to_dataframe(graph):\n",
        "    data = {'node_id': list(graph.nodes),\n",
        "            'latitude': [],\n",
        "            'longitude': [],\n",
        "            'street_count': [],\n",
        "            'degree_centrality': [],\n",
        "            'number_of_nearest_accidents': [],\n",
        "            'average_number_of_casualties': [],\n",
        "            'average_speed_limits': [],\n",
        "            'severity_category': []}\n",
        "\n",
        "    for node_id, node_data in graph.nodes(data=True):\n",
        "        data['latitude'].append(node_data['y'])\n",
        "        data['longitude'].append(node_data['x'])\n",
        "        data['street_count'].append(node_data['street_count'])\n",
        "        data['degree_centrality'].append(node_data['degree_centrality'])\n",
        "        data['number_of_nearest_accidents'].append(node_data['number_of_nearest_accidents'])\n",
        "        data['average_number_of_casualties'].append(node_data['average_number_of_casualties'])\n",
        "        data['average_speed_limits'].append(node_data['average_speed_limits'])\n",
        "        data['severity_category'].append(node_data['severity_category'])\n",
        "\n",
        "    df = pd.DataFrame.from_dict(data)\n",
        "    return df\n",
        "\n",
        "# Function that converts a graph into a pytorch geometric object\n",
        "def graph_to_pyg_data(graph1, target):\n",
        "\n",
        "    # \n",
        "    # We create the SubGraph dictionary that create pairs (node_id, count)\n",
        "    # The node_id si related to the entire graph and not to the subgraph in which\n",
        "    # the node indices are in the range [0, number of nodes in the subgraph]\n",
        "    node_id_to_index = {}\n",
        "    index_counter = 0\n",
        "    for node_id, node_data in graph1.nodes(data=True):\n",
        "        node_id_to_index[node_id] = index_counter\n",
        "        index_counter += 1\n",
        "\n",
        "    # We get the indices of the subgraph edges\n",
        "    edge_index = torch.tensor([edge[:2] for edge in graph1.edges]).t().contiguous()\n",
        "    \n",
        "    # The problem is that some nodes in the subgraph could have edges that are\n",
        "    # connected to the nodes of others subgraphs, so we remove these edges\n",
        "    #Filter edges\n",
        "    # We get the min and max of the nodes in the subgraph\n",
        "    min_node_id = min(graph1.nodes)\n",
        "    max_node_id = max(graph1.nodes)\n",
        "    \n",
        "    # We obtain the filtered edges but their indices are related to the entire graph\n",
        "    edge_index = filter_edges_by_node_indices(edge_index, min_node_id, max_node_id )\n",
        "\n",
        "    # We use the Subgraph dictionary to map the node_indices into the node_indices of subgraph\n",
        "    final_edge_index = edge_index.clone()\n",
        "    for i in range(edge_index.size(1)):\n",
        "        source_node_id = edge_index[0, i].item()\n",
        "        target_node_id = edge_index[1, i].item()\n",
        "\n",
        "        final_source_node_id = node_id_to_index[source_node_id]\n",
        "        final_target_node_id = node_id_to_index[target_node_id]\n",
        "\n",
        "        final_edge_index[0,i] = final_source_node_id\n",
        "        final_edge_index[1,i] = final_target_node_id\n",
        "\n",
        "    # The procedure also works in the case of the transformation of the entire graph into pytorch geometrci object\n",
        "        \n",
        "    node_features = [\n",
        "        [graph1.nodes[node]['y'],\n",
        "         graph1.nodes[node]['x'],\n",
        "         graph1.nodes[node]['street_count'],\n",
        "         graph1.nodes[node]['degree_centrality'],\n",
        "         graph1.nodes[node]['number_of_nearest_accidents'],\n",
        "         graph1.nodes[node]['average_number_of_casualties'],\n",
        "         graph1.nodes[node]['average_speed_limits']]\n",
        "\n",
        "        for node in graph1.nodes\n",
        "    ]\n",
        "\n",
        "    x = torch.tensor(node_features, dtype=torch.float)\n",
        "    y = torch.tensor(target, dtype=torch.long)\n",
        "    \n",
        "    data = Data(x=x, edge_index=final_edge_index, y=y)\n",
        "\n",
        "    return data\n",
        "\n",
        "# Function to filter the edges that are out from the set of nodes that we are considering\n",
        "def filter_edges_by_node_indices(edge_index, min_node_id, max_node_id):\n",
        "\n",
        "    count = 0\n",
        "    filtered_edge_index = torch.tensor([], dtype=torch.long)\n",
        "\n",
        "    for i in range(edge_index.size(1)):\n",
        "        source_node_id = edge_index[0, i].item()\n",
        "        target_node_id = edge_index[1, i].item()\n",
        "\n",
        "        valid_edge_mask = (\n",
        "            (source_node_id >= min_node_id) & \n",
        "            (source_node_id <= max_node_id) & \n",
        "            (target_node_id >= min_node_id) & \n",
        "            (target_node_id <= max_node_id)\n",
        "        )\n",
        "\n",
        "        # Update the filetered only if the mask is true\n",
        "        if valid_edge_mask:\n",
        "            current_edge = edge_index[:, i:i+1]\n",
        "            filtered_edge_index = torch.cat([filtered_edge_index, current_edge], dim=1)\n",
        "        \n",
        "    return filtered_edge_index\n",
        "\n",
        "# Conversion of Subgraphs in DataFrame in order to get the labels in the next step\n",
        "df_training = graph_to_dataframe(train_graph)\n",
        "df_validation = graph_to_dataframe(val_graph)\n",
        "df_test = graph_to_dataframe(test_graph)\n",
        "\n",
        "# We create a PyTorch Geometric data for each subgraph\n",
        "pyg_data_training = graph_to_pyg_data(train_graph, df_training['severity_category'])\n",
        "pyg_data_validation = graph_to_pyg_data(val_graph, df_validation['severity_category'])\n",
        "pyg_data_test = graph_to_pyg_data(test_graph, df_test['severity_category'])\n",
        "\n",
        "# We create a PyTorch Geometric data related to entire graph\n",
        "pyg_data = graph_to_pyg_data(graph, new_dataset['severity_category'])\n",
        "\n",
        "print(\"Pytorch Geometric object of the entire graph\")\n",
        "print(pyg_data)\n",
        "\n",
        "print(\"\\n\")\n",
        "print(\"Pytorch Geometric object of the training subgraph\")\n",
        "print(pyg_data_training)\n",
        "\n",
        "print(\"\\n\")\n",
        "print(\"Pytorch Geometric object of the validation subgraph\")\n",
        "print(pyg_data_validation)\n",
        "\n",
        "print(\"\\n\")\n",
        "print(\"Pytorch Geometric object of the test subgraph\")\n",
        "print(pyg_data_test)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pCNZfwouUCiu"
      },
      "source": [
        "## **1) TRAINING WITHOUT DIVISION IN BATCH  AND EVALUATION**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "M5o6Tc7QUCiu",
        "outputId": "c82344c1-87a8-4cd8-a9e0-09ffd19c1064"
      },
      "outputs": [],
      "source": [
        "\n",
        "from sklearn.metrics import precision_score\n",
        "\n",
        "# Function to train the model\n",
        "def train():\n",
        "      model.train()\n",
        "      optimizer.zero_grad()\n",
        "      out = model(pyg_data_training.x, pyg_data_training.edge_index)\n",
        "      loss = criterion(out, pyg_data_training.y) \n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      return loss\n",
        "\n",
        "# Function to validate the model\n",
        "def val():\n",
        "      model.eval()\n",
        "      with torch.no_grad():\n",
        "        out = model(pyg_data_validation.x, pyg_data_validation.edge_index)\n",
        "        predictions = torch.argmax(out, dim=1)\n",
        "        correct_predictions = predictions.eq(pyg_data_validation.y)\n",
        "        test_accuracy = correct_predictions.sum().item() / len(pyg_data_validation.y)\n",
        "\n",
        "        precision = precision_score(correct_predictions, predictions, average='weighted')\n",
        "        recall = recall_score(correct_predictions, predictions, average='weighted')\n",
        "        f1 = f1_score(correct_predictions, predictions, average='weighted')\n",
        "\n",
        "        return test_accuracy, precision, recall, f1\n",
        "\n",
        "# Function to test the model\n",
        "def test():\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        out = model(pyg_data_test.x, pyg_data_test.edge_index)\n",
        "        predictions = torch.argmax(out, dim=1)\n",
        "        correct_predictions = predictions.eq(pyg_data_test.y)\n",
        "        test_accuracy = correct_predictions.sum().item() / len(pyg_data_test.y)\n",
        "\n",
        "        precision = precision_score(correct_predictions, predictions, average='weighted')\n",
        "        recall = recall_score(correct_predictions, predictions, average='weighted')\n",
        "        f1 = f1_score(correct_predictions, predictions, average='weighted')\n",
        "\n",
        "    return test_accuracy, precision, recall, f1\n",
        "\n",
        "for epoch in range(1, 200):\n",
        "    loss = train()\n",
        "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}')\n",
        "\n",
        "# We use out val function\n",
        "accuracy, precision, recall, f1 = val()\n",
        "print(\"Validation\")\n",
        "print(f'Test Accuracy: {accuracy}')\n",
        "print(f'Precision: {precision:.4f}')\n",
        "print(f'Recall: {recall:.4f}')\n",
        "print(f'F1 Score: {f1:.4f}')\n",
        "\n",
        "# We use out test function\n",
        "print(\"Test\")\n",
        "accuracy, precision, recall, f1 = test()\n",
        "\n",
        "print(f'Test Accuracy: {accuracy}')\n",
        "print(f'Precision: {precision:.4f}')\n",
        "print(f'Recall: {recall:.4f}')\n",
        "print(f'F1 Score: {f1:.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **2) TRAINING WITH DIVISION IN BATCH AND EVALUATION**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch_geometric.data import DataLoader, Batch  # Import Batch\n",
        "\n",
        "# Convert the training pytorch geometric object into a list\n",
        "train_dataset = [pyg_data_training] \n",
        "\n",
        "# Batch size for training\n",
        "batch_size = 133\n",
        "\n",
        "# Custom collate function to convert Data objects into tensors\n",
        "def collate(data_list):\n",
        "    batch = Batch.from_data_list(data_list)  \n",
        "    return batch.x, batch.edge_index, batch.y\n",
        "\n",
        "# Create DataLoader for training with the custom collate function\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate)\n",
        "\n",
        "# Definition of the training function\n",
        "def train():\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    for batch in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        #batch_x, batch_edge_index, batch_y = batch.x, batch.edge_index, batch.y\n",
        "\n",
        "        out = model(batch.x, batch.edge_index)\n",
        "        loss = criterion(out, batch.y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(train_loader)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(200):\n",
        "\n",
        "     # Shuffle the training data for each epoch\n",
        "    train_loader = DataLoader([pyg_data_training], batch_size=133, shuffle=True, collate_fn=collate)\n",
        "\n",
        "    train_loss = train()\n",
        "\n",
        "    print(f'Epoch: {epoch:03d}, Loss: {train_loss:.4f}')\n",
        "\n",
        "   \n",
        "# We use out val function\n",
        "accuracy, precision, recall, f1 = val()\n",
        "print(\"Validation\")\n",
        "print(f'Test Accuracy: {accuracy}')\n",
        "print(f'Precision: {precision:.4f}')\n",
        "print(f'Recall: {recall:.4f}')\n",
        "print(f'F1 Score: {f1:.4f}')\n",
        "\n",
        "print(\"Test\")\n",
        "# We use out test function\n",
        "accuracy, precision, recall, f1 = test()\n",
        "\n",
        "print(f'Test Accuracy: {accuracy}')\n",
        "print(f'Precision: {precision:.4f}')\n",
        "print(f'Recall: {recall:.4f}')\n",
        "print(f'F1 Score: {f1:.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_OZY96y-UCiu"
      },
      "source": [
        "## **OUTPUT EMBEDING OF OUR TRAINING MODEL**\n",
        "Let us visualizes the 2D embeddings produced by our trained GNN. \n",
        "\n",
        "The colors in the plot represent different node labels, providing insight into how well the GCN has grouped nodes with similar labels in the embedding space"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j0vCVnfMUCiv"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "# Function to visualize\n",
        "def visualize(h, color):\n",
        "    z = TSNE(n_components=2).fit_transform(h.detach().cpu().numpy())\n",
        "\n",
        "    plt.figure(figsize=(10,10))\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "\n",
        "    plt.scatter(z[:, 0], z[:, 1], s=10, c=color, cmap=\"Set2\")\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# We assume that color is a PyTorch tensor\n",
        "model.eval()\n",
        "out = model(pyg_data_test.x, pyg_data_test.edge_index)\n",
        "visualize(out, color=pyg_data_test.y)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
